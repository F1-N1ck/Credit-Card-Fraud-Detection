============================
Random Forest Results:
============================
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85295
           1       0.97      0.70      0.82       148

    accuracy                           1.00     85443
   macro avg       0.99      0.85      0.91     85443
weighted avg       1.00      1.00      1.00     85443

============================
Gradient Boosting Results:
============================
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85295
           1       0.73      0.16      0.27       148

    accuracy                           1.00     85443
   macro avg       0.86      0.58      0.63     85443
weighted avg       1.00      1.00      1.00     85443

============================
XGBoost Results:
============================
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85295
           1       0.78      0.80      0.79       148

    accuracy                           1.00     85443
   macro avg       0.89      0.90      0.90     85443
weighted avg       1.00      1.00      1.00     85443

============================
Conclusion:
============================

Among the three models tested, Random Forest and XGBoost performed the best for detecting fraudulent transactions.

- **Random Forest** achieved very high precision (0.97) and good recall (0.70) on fraud cases, showing it is robust and balanced.
- **XGBoost** slightly outperformed in recall (0.80) while maintaining strong precision (0.78), making it excellent for reducing false negatives in fraud detection scenarios.
- **Gradient Boosting** had lower recall (0.16), making it less effective for this highly imbalanced task.

Overall, XGBoost appears to be the most suitable choice when prioritizing recall (catching as many frauds as possible), whereas Random Forest also remains a strong and interpretable baseline.
